{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.polynomial import Polynomial\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from shapely.ops import unary_union, cascaded_union\n",
    "from shapely.validation import make_valid\n",
    "import folium\n",
    "import contextily as ctx\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Point, MultiPolygon, Polygon\n",
    "from shapely.ops import unary_union\n",
    "import geopandas.tools\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPS - DHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Directory containing the GeoJSON files\n",
    "geojson_dir = '/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/maps-master/LGD Villages/'\n",
    "\n",
    "# List all GeoJSON files in the directory\n",
    "geojson_files = [os.path.join(geojson_dir, file) for file in os.listdir(geojson_dir) if file.endswith('.geojson')]\n",
    "\n",
    "# Load all GeoJSON files into a list of GeoDataFrames\n",
    "gdfs = [gpd.read_file(file) for file in geojson_files]\n",
    "\n",
    "# Concatenate all GeoDataFrames into a single GeoDataFrame\n",
    "merged_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_gdf2 = merged_gdf.copy()\n",
    "# Drop the specified columns\n",
    "columns_to_drop = [\n",
    "    'OBJECTID', 'vilnam_soi', 'vilcode11', 'vilname11', 'vil_lgd', \n",
    "    'block_name', 'block_lgd', 'Remark', \n",
    "    'ac_no', 'test', \n",
    "]\n",
    "merged_gdf2 = merged_gdf2.drop(columns=columns_to_drop)\n",
    "\n",
    "# Function to clean invalid geometries\n",
    "def clean_geometry(geom):\n",
    "    if not geom.is_valid:\n",
    "        return make_valid(geom)\n",
    "    return geom\n",
    "\n",
    "# Clean invalid geometries\n",
    "merged_gdf2['geometry'] = merged_gdf2['geometry'].apply(clean_geometry)\n",
    "\n",
    "# Optional: further clean geometries by buffering and unbuffering (can help with slight geometry errors)\n",
    "merged_gdf2['geometry'] = merged_gdf2.buffer(0)\n",
    "\n",
    "# Define the aggregation functions\n",
    "agg_functions = {\n",
    "    'shape_Length': 'sum',\n",
    "    'shape_Area': 'sum'\n",
    "    # Add more columns with 'first' if you want to retain them without aggregation\n",
    "}\n",
    "\n",
    "# Add all other columns with 'first' to retain their first occurrence\n",
    "for col in merged_gdf2.columns:\n",
    "    if col not in agg_functions and col != 'geometry':\n",
    "        agg_functions[col] = 'first'\n",
    "\n",
    "# Dissolve (merge) villages into subdistricts based on the \"subdt_lgd\" attribute, summing 'shape_Length' and 'shape_Area'\n",
    "dissolved_gdf = merged_gdf2.dissolve(by=['stname','dtname','sdtname','gp_code', 'gp_name',], aggfunc=agg_functions)\n",
    "\n",
    "\n",
    "# Optionally, save the dissolved GeoDataFrame to a new GeoJSON file\n",
    "# output_file = os.path.join(geojson_dir, 'merged_subdistricts.geojson')\n",
    "# dissolved_gdf.to_file(output_file, driver='GeoJSON')\n",
    "\n",
    "# print(\"Merged and dissolved GeoDataFrame saved to:\", output_file)\n",
    "dissolved_gdf = dissolved_gdf.drop(columns={'stname','dtname','sdtname','gp_code', 'gp_name',})\n",
    "dissolved_gdf = dissolved_gdf.reset_index()\n",
    "\n",
    "ts = dissolved_gdf[dissolved_gdf['stname']=='TELANGANA']\n",
    "\n",
    "\n",
    "dissolved_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = dissolved_gdf.plot(figsize=(10, 10), edgecolor='k', alpha=0.5)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Dissolved GeoDataFrame Plot')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Optional: Add a legend if you have categorical data or specific attributes to show\n",
    "# plt.legend(title='Your Legend Title')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfhs5 = gpd.read_file('/Users/sid/Desktop/IAGE7AFL/IAGE7AFL.shp')\n",
    "india = gpd.read_file('/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/maps-master/Country/india.geojson')\n",
    "\n",
    "# Ensure both GeoDataFrames have the same CRS\n",
    "if india.crs != nfhs5.crs:\n",
    "    nfhs5 = nfhs5.to_crs(india.crs)\n",
    "\n",
    "# Perform spatial join to keep only points within India\n",
    "nfhs5_within_india = gpd.sjoin(nfhs5, india, op='within')\n",
    "\n",
    "# Dropping the 'index_right' column created by the spatial join\n",
    "nfhs5_within_india = nfhs5_within_india.drop(columns='index_right')\n",
    "\n",
    "# Save or further process the filtered GeoDataFrame\n",
    "# nfhs5_within_india.to_file('path_to_filtered_nfhs5_shapefile')\n",
    "\n",
    "print(f\"Number of points within India: {len(nfhs5_within_india)}\")\n",
    "print(f\"Number of points outside India: {len(nfhs5) - len(nfhs5_within_india)}\")\n",
    "\n",
    "buffer_distance = 0.04  # 4 km \n",
    "nfhs5_within_india['geometry'] = nfhs5_within_india.geometry.buffer(buffer_distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform spatial join to identify intersections\n",
    "print(\"Performing spatial join...\")\n",
    "joined_gdf2 = gpd.sjoin(dissolved_gdf, nfhs5_within_india, how='inner', op='intersects')\n",
    "\n",
    "# Calculate the area of each village\n",
    "print(\"Calculating village areas...\")\n",
    "dissolved_gdf['village_area'] = dissolved_gdf.area\n",
    "\n",
    "# Calculate the area of intersection for each village with nfhs5_within_india polygons\n",
    "print(\"Calculating intersection areas...\")\n",
    "intersections = []\n",
    "for i in tqdm(dissolved_gdf.index, desc=\"Overlay operation\"):\n",
    "    village = dissolved_gdf.loc[[i]]\n",
    "    intersection = gpd.overlay(village, nfhs5_within_india, how='intersection')\n",
    "    intersection['village_area'] = village['village_area'].values[0]\n",
    "    intersection['intersection_area'] = intersection.area\n",
    "    intersections.append(intersection)\n",
    "\n",
    "# Concatenate the list of GeoDataFrames into a single GeoDataFrame\n",
    "intersections_gdf = gpd.GeoDataFrame(pd.concat(intersections, ignore_index=True))\n",
    "\n",
    "# Calculate the proportion of each village's area that lies within each intersecting nfhs5_within_india polygon\n",
    "print(\"Calculating proportions...\")\n",
    "intersections_gdf['proportion'] = intersections_gdf['intersection_area'] / intersections_gdf['village_area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the polygon with the highest proportion for each village\n",
    "print(\"Selecting highest proportion for each village...\")\n",
    "intersections_gdf = intersections_gdf.sort_values(by='proportion', ascending=False).drop_duplicates(subset=['stname', 'dtname', 'sdtname', 'gp_name', 'gp_code'], keep='first')\n",
    "intersections_gdf.columns\n",
    "Final_gp_df = intersections_gdf.drop(columns={'shape_Length','shape_Area', \n",
    "       'ADM1FIPS', 'ADM1FIPSNA', 'ADM1SALBNA', 'ADM1SALBCO', 'ADM1DHS',\n",
    "       'URBAN_RURA','LONGNUM', 'ALT_GPS', 'ALT_DEM', 'DATUM', 'geometry',})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final_gp_df.to_csv('/Users/sid/Desktop/DHS/dhs_gp_prop.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ax = nfhs5_within_india.plot(figsize=(10, 10), edgecolor='k', alpha=0.5)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Dissolved GeoDataFrame Plot')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "\n",
    "# Optional: Add a legend if you have categorical data or specific attributes to show\n",
    "# plt.legend(title='Your Legend Title')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the CRS is in WGS84 for Folium\n",
    "\n",
    "# Create a folium map centered on India\n",
    "m = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\n",
    "\n",
    "\n",
    "# Add the nfhs5_within_india GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    intersections_gdf,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'none',  # No fill\n",
    "        'color': 'blue',  # Border color\n",
    "        'weight': 1.5,  # Border width\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add the dissolved_gdf GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    ts,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'none',  # No fill\n",
    "        'color': 'red',  # Border color\n",
    "        'weight': 1.5,  # Border width\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add tile layer for background\n",
    "folium.TileLayer('Stamen Terrain').add_to(m)\n",
    "folium.TileLayer('Stamen Toner').add_to(m)\n",
    "folium.TileLayer('Stamen Watercolor').add_to(m)\n",
    "folium.TileLayer('cartodb positron').add_to(m)\n",
    "folium.TileLayer('cartodb dark_matter').add_to(m)\n",
    "folium.TileLayer('openstreetmap').add_to(m)\n",
    "folium.LayerControl().add_to(m)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the CRS is in WGS84 for Folium\n",
    "nfhs5_within_india = nfhs5_within_india.to_crs(epsg=4326)\n",
    "\n",
    "# Create a folium map centered on India\n",
    "m = folium.Map(location=[20.5937, 78.9629], zoom_start=5)\n",
    "\n",
    "\n",
    "# Add the nfhs5_within_india GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    nfhs5_within_india,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'none',  # No fill\n",
    "        'color': 'blue',  # Border color\n",
    "        'weight': 1.5,  # Border width\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add the dissolved_gdf GeoDataFrame to the map\n",
    "folium.GeoJson(\n",
    "    ts,\n",
    "    style_function=lambda feature: {\n",
    "        'fillColor': 'none',  # No fill\n",
    "        'color': 'red',  # Border color\n",
    "        'weight': 1.5,  # Border width\n",
    "    }\n",
    ").add_to(m)\n",
    "\n",
    "# Add tile layer for background\n",
    "folium.TileLayer('Stamen Terrain').add_to(m)\n",
    "folium.TileLayer('Stamen Toner').add_to(m)\n",
    "folium.TileLayer('Stamen Watercolor').add_to(m)\n",
    "folium.TileLayer('cartodb positron').add_to(m)\n",
    "folium.TileLayer('cartodb dark_matter').add_to(m)\n",
    "folium.TileLayer('openstreetmap').add_to(m)\n",
    "folium.LayerControl().add_to(m)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_gdf = gpd.sjoin(nfhs5, dissolved_gdf, how='left', op='within')\n",
    "columns_to_drop = [\n",
    "    'DHSID','DHSCC','DHSYEAR','CCFIPS',\n",
    " 'ADM1FIPS',\n",
    " 'ADM1FIPSNA',\n",
    " 'ADM1SALBNA',\n",
    " 'ADM1SALBCO',\n",
    " 'ADM1DHS',\n",
    " 'geometry',\n",
    "#  'index_right',\n",
    "]\n",
    "\n",
    "joined_gdf = joined_gdf.drop(columns=columns_to_drop)\n",
    "\n",
    "joined_gdf = joined_gdf.dropna(subset=['stname'])\n",
    "polygons_with_points = joined_gdf[['index_right', 'gp_name','gp_code']].drop_duplicates()\n",
    "\n",
    "# joined_gdf.to_csv('/Users/sid/Desktop/nfhs_gp.csv',index=False)\n",
    "\n",
    "joined_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your .dta file\n",
    "file_path = '/Users/sid/Desktop/DHS/BR/BR.DTA'\n",
    "\n",
    "# Step 1: Read the Stata file to get the column names\n",
    "initial_df = pd.read_stata(file_path, convert_categoricals=False)\n",
    "\n",
    "# Identify the columns to drop (for example, 'SDIST')\n",
    "columns_to_drop = ['sdist']\n",
    "\n",
    "# Step 2: Read the Stata file again, dropping the problematic column(s) and converting categoricals\n",
    "df = pd.read_stata(file_path, convert_categoricals=True, columns=[col for col in initial_df.columns if col not in columns_to_drop])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df['v602'].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.dropna(axis=1, how='all')\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned2 = df_cleaned[df_cleaned['b2']>=2017]\n",
    "df_cleaned['period'] = np.where(df_cleaned['b2'] < 2017, 0, \n",
    "                                    np.where(df_cleaned['b2'] >= 2017, 1, np.nan))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned2.to_csv('/Users/sid/Desktop/DHS/BR/BR_python_filter.csv',index=False)\n",
    "df_cleaned.to_csv('/Users/sid/Desktop/DHS/BR/BR_python_all.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DF read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned2 = pd.read_csv('/Users/sid/Desktop/DHS/BR/BR_python_filter.csv')\n",
    "df_cleaned = pd.read_csv('/Users/sid/Desktop/DHS/BR/BR_python_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_cleaned_filter = df_cleaned[['v001','v002',\n",
    "    's361', 'v106', \n",
    "    'v133', 'v149', 'v701','s365a','s365b' ,'s365c', 's365d', 's365e', 's365g', 's365j', \n",
    "    's365o', 's365n', 's365p', 's365q', 's365r', 's566a','s566b','s566c', 'v602', 'v201', \n",
    "    'v613', 's370a', 's370b','s370c', 's370d','s370e', 's370f','s370g', 's370h', 's370i', \n",
    "    's370j','s370k', 's370l','v010', 'v009', 'v212', 's929', 'v130', 'v131','v739', 'v741',  \n",
    "    'v714','v719', 'v731', 'v741', 'm2h','m2g', 'm2i','m2j','m2k', 'period','b2'\n",
    "]]\n",
    "\n",
    "df_cleaned_filter2 = df_cleaned_filter.rename(columns={\n",
    "                        'v001':'Cluster number','v002':'Household number','v739':'Person who decides how to spend respondents earnings', \n",
    "                        'v741':'Type of earnings from respondents work', 'v714':'Respondent currently working', 'v719':'Respondent works for family, others, self', \n",
    "                        'v731':'Respondent worked in last 12 months', 'm2h':'Prenatal: community/village health worker',\n",
    "                        'm2g':'Prenatal: DAI/traditional birth attendant' ,'m2i':'Prenatal: anganwadi/ICDS worker', 'm2j':'Prenatal: ASHA', 'm2k':'Prenatal: other',\n",
    "                        's361':'Met with an anganwadi worker, ASHA or other community health worker in last 3 mo', 'v106':'Highest educational level', \n",
    "                        'v133':'Education in single years', 'v149':'Educational attainment', 'v701':'Husband education level',\n",
    "                        's365a': 'Services: Family Planning', 's365b': 'Services: Immunization', 's365c': 'Services: Antenatal Care', 's365d': 'Services: Delivery Care',\n",
    "                        's365e': 'Services: Birth Preparedness', 's365g': 'Services: Postnatal Care', 's365j': 'Services: Treatment For Sick Child',\n",
    "                        's365o': 'Services: Early Childhood Care', 's365n': 'Services: Growth Monitoring Of Child', 's365p': 'Services: Pre-School Education',\n",
    "                        's365q': 'Services: Nutrition/Health Education', 's365r': 'Services: Family Life Education',\n",
    "                        's566a':'Benefits:Supplementary food','s566b':'Benefits:Health check-ups','s566c':'Benefits:Health and nutrition education', \n",
    "                        'v602':'Fertility preference', 'v201':'Total children ever born', 'v613':'Ideal number of children', \n",
    "                        's370a': 'Service went for: Family Planning', 's370b': 'Service went for: Immunization', 's370c': 'Service went for: Antenatal Care',\n",
    "                        's370d': 'Service went for: Delivery Care', 's370e': 'Service went for: Postnatal Care', 's370f': 'Service went for: Disease Prevention',\n",
    "                        's370g': 'Service went for: Medical Treatment For Self', 's370h': 'Service went for: Treatment For Child', \n",
    "                        's370i': 'Service went for: Treatment For Other Person',  's370j': 'Service went for: Growth Monitoring Of Child',\n",
    "                        's370k': 'Service went for: Health Check-Up', 's370l': 'Service went for: Medical Termination Of Pregnancy',\n",
    "                        'v010':'Respondents year of birth', 'v009':'Respondents month of birth', 'v212':'Age of respondent at 1st birth', 's929':'Independent money decision', \n",
    "                        'v130':'Religion', 'v131':'Ethnicity','b2':'Child birth year'\n",
    "                                    })\n",
    "df_cleaned_filter2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned_filter2 = df_cleaned_filter2.astype('object')\n",
    "df_cleaned_filter2['Cluster number'] = df_cleaned_filter2['Cluster number'].astype('int')\n",
    "df_cleaned_filter2['Household number'] = df_cleaned_filter2['Household number'].astype('int')\n",
    "df_cleaned_filter2['Education in single years'] = df_cleaned_filter2['Education in single years'].astype('int')\n",
    "df_cleaned_filter2['Ideal number of children'] = pd.to_numeric(df_cleaned_filter2['Ideal number of children'], errors='coerce')\n",
    "df_cleaned_filter2['Total children ever born'] = df_cleaned_filter2['Total children ever born'].astype('Int64')\n",
    "df_cleaned_filter2['Respondents year of birth'] = df_cleaned_filter2['Respondents year of birth'].astype('Int64')\n",
    "df_cleaned_filter2['Respondents month of birth'] = df_cleaned_filter2['Respondents month of birth'].astype('Int64')\n",
    "df_cleaned_filter2['Age of respondent at 1st birth'] = df_cleaned_filter2['Age of respondent at 1st birth'].astype('Int64')\n",
    "df_cleaned_filter2['period'] = df_cleaned_filter2['period'].astype('object')\n",
    "print(df_cleaned_filter2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_cleaned_filter2.to_csv('/Users/sid/Desktop/DHS/BR/BR_python_filter.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/Users/sid/Desktop/DHS/dhs_subd.csv')\n",
    "gp = pd.read_csv('/Users/sid/Desktop/DHS/dhs_gp.csv')\n",
    "prop = pd.read_csv('/Users/sid/Desktop/DHS/dhs_gp_prop.csv')\n",
    "dhs = pd.read_csv('/Users/sid/Desktop/DHS/BR/BR_python_filter.csv')\n",
    "meg_sorted = pd.read_csv('/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/ma2020/2020_Villages_sorted.csv')\n",
    "lum12_sorted = pd.read_csv('/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/ma2020/2011_Villages_sorted.csv')\n",
    "# df_cleaned = pd.read_csv('/Users/sid/Desktop/DHS/BR/BR_python_all.csv')\n",
    "\n",
    "lum12_sorted['sc_pop_share']=lum12_sorted['sc_pop']/lum12_sorted['population']\n",
    "lum12_sorted['st_pop_share']=lum12_sorted['st_pop']/lum12_sorted['population']\n",
    "meg_sorted['sc_pop'] = None\n",
    "meg_sorted['st_pop'] = None\n",
    "meg_sorted['sc_pop_share'] = None\n",
    "meg_sorted['st_pop_share'] = None\n",
    "\n",
    "meg_sorted['year']=2020\n",
    "lum12_sorted['year']=2011\n",
    "\n",
    "\n",
    "# Handle duplicate column names by keeping the first occurrence\n",
    "meg_sorted = meg_sorted.loc[:, ~meg_sorted.columns.duplicated()]\n",
    "\n",
    "# Check for unique column names\n",
    "if not meg_sorted.columns.is_unique or not lum12_sorted.columns.is_unique:\n",
    "    raise ValueError(\"One of the DataFrames has non-unique column names.\")\n",
    "\n",
    "# Find the common columns\n",
    "common_columns = meg_sorted.columns.intersection(lum12_sorted.columns)\n",
    "\n",
    "# Subset both DataFrames to include only these common columns\n",
    "meg20_common = meg_sorted[common_columns]\n",
    "lum11_common = lum12_sorted[common_columns]\n",
    "\n",
    "gp = gp[['DHSCLUST','stname','dtname','sdtname','gp_code','gp_name','dist_lgd','state_lgd','subdt_lgd']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meg20 = meg20_common[['state','district','subdistrict','gp','gpcode', 'vil_code',\n",
    "                      'Village','state_lgd','dist_lgd','subdt_lgd','village_area_sqkm',\n",
    "                      'SubDistrict Area', 'avg_ntl','population', 'male_pop', 'female_pop', \n",
    "                      'sc_pop', 'st_pop', 'year', 'p_school','m_school', \n",
    "                      'h_school', 'ssc_school', 'arts_and_science_degree_college','iti', \n",
    "                      'phc', 'dispensary', 'mcwc', 'veterinary_hospital', 'aanganwadi',\n",
    "                      'cooperative_bank', 'post_office', 'bus', 'railway', 'all_weather_road','mandis'\n",
    "       ]]\n",
    "\n",
    "lum11 = lum11_common[['state','district','subdistrict','gp','gpcode', 'vil_code',\n",
    "                      'Village','state_lgd','dist_lgd','subdt_lgd','village_area_sqkm',\n",
    "                      'SubDistrict Area', 'avg_ntl','population', 'male_pop', 'female_pop', \n",
    "                      'sc_pop', 'st_pop', 'year', 'p_school','m_school', \n",
    "                      'h_school', 'ssc_school', 'arts_and_science_degree_college','iti', \n",
    "                      'phc', 'dispensary', 'mcwc', 'veterinary_hospital', 'aanganwadi',\n",
    "                      'cooperative_bank', 'post_office', 'bus', 'railway', 'all_weather_road','mandis'\n",
    "       ]]\n",
    "\n",
    "meg20 = meg20.drop(columns='vil_code')\n",
    "lum11 = lum11.drop(columns='vil_code')\n",
    "\n",
    "\n",
    "# Assuming meg20 is your DataFrame\n",
    "columns_to_exclude = ['state_lgd', 'dist_lgd', 'subdt_lgd', 'SubDistrict Area','year']\n",
    "group_columns = ['state', 'district', 'subdistrict', 'gp', 'gpcode','Village']\n",
    "\n",
    "# Create a dictionary for aggregation\n",
    "agg_dict = {col: 'sum' for col in meg20.columns if col not in columns_to_exclude + group_columns}\n",
    "# For the excluded columns, use 'first' to keep the first occurrence\n",
    "for col in columns_to_exclude:\n",
    "    agg_dict[col] = 'first'\n",
    "\n",
    "# Perform the groupby and aggregation\n",
    "meg = meg20.groupby(['state', 'district', 'subdistrict', 'gp', 'gpcode']).agg(agg_dict).reset_index()\n",
    "lum = lum11.groupby(['state', 'district', 'subdistrict', 'gp', 'gpcode']).agg(agg_dict).reset_index()\n",
    "\n",
    "# meg['year']=2020\n",
    "# meg = meg.assign(year=2020)\n",
    "# meg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dhs_gp_join = pd.merge(dhs,gp,left_on='Cluster number',right_on='DHSCLUST',how='left')\n",
    "dhs_gp_join = dhs_gp_join.dropna(subset=['DHSCLUST','gp_name'])\n",
    "dhs_gp_join = dhs_gp_join.rename(columns={\n",
    "                                        'stname':'state',\n",
    "                                        'dtname':'district',\n",
    "                                        'sdtname':'subdistrict',\n",
    "                                        'gp_name':'gp',\n",
    "                                        'gp_code':'gpcode',\n",
    "                                        })\n",
    "\n",
    "dhs_post = dhs_gp_join[dhs_gp_join['period'] == 1 ]\n",
    "dhs_pre = dhs_gp_join[dhs_gp_join['period'] == 0 ]\n",
    "\n",
    "\n",
    "dhs_ma_post = pd.merge(dhs_post,meg,on=['state','state_lgd', 'district', 'dist_lgd',\n",
    "                                            'subdistrict', 'subdt_lgd','gp', 'gpcode'],how='left')\n",
    "dhs_ma_pre = pd.merge(dhs_pre,lum,on=['state','state_lgd', 'district', 'dist_lgd',\n",
    "                                            'subdistrict', 'subdt_lgd','gp', 'gpcode'],how='left')\n",
    "dhs_ma_join = pd.concat([dhs_ma_post,dhs_ma_pre])\n",
    "dhs_ma_join = dhs_ma_join.dropna(subset=['year'])\n",
    "dhs_ma_join\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
