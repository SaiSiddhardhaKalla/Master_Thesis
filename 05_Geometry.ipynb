{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry& Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run-time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Load state boundary shapefile and CSV with village points\n",
    "state_boundary = gpd.read_file('/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/INDIAN-SHAPEFILES-master/State Boundary/KARNATAKA_STATE.geojson')\n",
    "villages = gpd.read_file('/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/MA/MA Data -  All India All_State/29.csv')\n",
    "print(len(villages))\n",
    "\n",
    "# Load the GeoJSON file with urban city points\n",
    "geojson_file = '//Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/INDIAN-SHAPEFILES-master/Subdist_hq/KARNATAKA Sub District Hq.geojson'\n",
    "urban_cities = gpd.read_file(geojson_file)\n",
    "\n",
    "# Ensure the index of urban_cities is consistent and numeric\n",
    "urban_cities.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "def create_geometry(row):\n",
    "    try:\n",
    "        longitude = float(row['village_longitude'])\n",
    "        latitude = float(row['village_latitude'])\n",
    "        return Point(longitude, latitude)\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "villages['geometry'] = villages.apply(create_geometry, axis=1)\n",
    "\n",
    "# Perform spatial join\n",
    "joined = gpd.sjoin(villages, state_boundary, how='left', op='within')\n",
    "\n",
    "# Filter and replace points outside the state boundary\n",
    "def replace_latitude(row):\n",
    "    if row['index_right'] >= 0:\n",
    "        return row['village_latitude']\n",
    "    return None\n",
    "\n",
    "def replace_longitude(row):\n",
    "    if row['index_right'] >= 0:\n",
    "        return row['village_longitude']\n",
    "    return None\n",
    "\n",
    "joined['village_latitude'] = joined.apply(replace_latitude, axis=1)\n",
    "joined['village_longitude'] = joined.apply(replace_longitude, axis=1)\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    R = 6371.0  # Radius of the Earth in kilometers\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    a = sin(dlat / 2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def find_nearest_urban_centre(row):\n",
    "    if row['geometry'] is None:\n",
    "        return None, None\n",
    "    \n",
    "    distances = urban_cities.geometry.apply(lambda point: haversine(row['geometry'].x, row['geometry'].y, point.x, point.y))\n",
    "    nearest_city_index = distances.idxmin()\n",
    "    nearest_city_name = urban_cities.loc[nearest_city_index, 'LOC_NAME']\n",
    "    nearest_city_proximity= distances.loc[nearest_city_index]\n",
    "    return nearest_city_name, nearest_city_proximity\n",
    "\n",
    "joined[['nearest_urban_centre', 'nearest_urban_distance']] = joined.apply(find_nearest_urban_centre, axis=1, result_type='expand')\n",
    "\n",
    "# Save the updated CSV with null values for points outside boundary\n",
    "joined.drop(columns=['village_latitude', 'village_longitude','index_right', 'STNAME', 'STCODE11', 'STNAME_SH', 'Shape_Length', 'Shape_Area', 'OBJECTID', 'geometry'], inplace=True)\n",
    "print(len(joined))\n",
    "\n",
    "joined.to_csv('/Users/sid/Desktop/KA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# Specify the directory containing CSV files\n",
    "folder_path = '/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/ma2020/states'\n",
    "\n",
    "# List all CSV files in the folder\n",
    "csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty DataFrame to store concatenated data\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "# Loop through CSV files and concatenate them\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    data = pd.read_csv(file_path)\n",
    "    all_data = pd.concat([all_data, data], ignore_index=True)\n",
    "\n",
    "# Save the concatenated data to a new CSV file\n",
    "output_file = '/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/ma2020/concatenated_data.csv'\n",
    "\n",
    "all_data['nearest_urban_distance'] = np.where(all_data['nearest_urban_distance'] > 250, np.nan, all_data['nearest_urban_distance'])\n",
    "\n",
    "\n",
    "all_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Concatenated data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = '/Users/sid/Desktop/ma2020/concatenated_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Select columns from 16 to second last\n",
    "numeric_columns = data.columns[16:-2]\n",
    "\n",
    "# Convert selected columns to numeric, handling errors\n",
    "data[numeric_columns] = data[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop rows with any missing values in selected columns\n",
    "data.dropna(subset=numeric_columns, inplace=True)\n",
    "\n",
    "print(f\"Rows remaining after dropping missing values: {len(data)}\")\n",
    "\n",
    "# Create a new DataFrame for transposed data\n",
    "transposed_data = pd.DataFrame()\n",
    "\n",
    "# Extract columns from 16 to end and transpose them\n",
    "columns_to_transpose = data.columns[16:-2]\n",
    "numeric_columns_to_transpose = [col for col in columns_to_transpose if pd.api.types.is_numeric_dtype(data[col])]\n",
    "\n",
    "transposed_data['Variable'] = numeric_columns_to_transpose\n",
    "transposed_data['Range'] = transposed_data['Variable'].apply(lambda col: data[col].max() - data[col].min() if col in data else None)\n",
    "transposed_data['Max'] = transposed_data['Variable'].apply(lambda col: data[col].max() if col in data else None)\n",
    "transposed_data['Min'] = transposed_data['Variable'].apply(lambda col: data[col].min() if col in data else None)\n",
    "transposed_data['Average'] = transposed_data['Variable'].apply(lambda col: data[col].mean() if col in data else None)  # Add Average column\n",
    "transposed_data['Median'] = transposed_data['Variable'].apply(lambda col: data[col].median() if col in data else None)  # Add Median column\n",
    "\n",
    "# Save the summary data to a new CSV file\n",
    "output_transposed_file = '/Users/sid/Desktop/ma2020/summary_data.csv'\n",
    "transposed_data.to_csv(output_transposed_file, index=False)\n",
    "\n",
    "print(f\"Transposed data saved to {output_transposed_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Village area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.ops import transform\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load the shapefile\n",
    "shapefile_path = '/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/VIIRS_Monthly_Tiled/2018/tmpif4jtczm.shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Define the geodetic coordinate system (EPSG:4326)\n",
    "geod = pyproj.Geod(ellps='WGS84')\n",
    "\n",
    "def calculate_area(row):\n",
    "    geometry = row['geometry']\n",
    "    if geometry.geom_type == 'Polygon':\n",
    "        polygons = [geometry]\n",
    "    elif geometry.geom_type == 'MultiPolygon':\n",
    "        polygons = geometry.geoms\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    area = 0.0\n",
    "    for polygon in polygons:\n",
    "        lon, lat = polygon.centroid.x, polygon.centroid.y\n",
    "        vertices = list(polygon.exterior.coords)\n",
    "        for i in range(len(vertices) - 1):\n",
    "            lon1, lat1 = vertices[i]\n",
    "            lon2, lat2 = vertices[i + 1]\n",
    "            _, _, distance = geod.inv(lon1, lat1, lon2, lat2)\n",
    "            area += lat1 * lat2 * distance\n",
    "    return abs(area) / 2.0 / 1e6\n",
    "\n",
    "# Calculate the area in square kilometers\n",
    "gdf['area_sq_km'] = gdf.apply(calculate_area, axis=1)\n",
    "\n",
    "# # Create a new DataFrame with desired columns\n",
    "# new_df = gdf[['attribute_column1', 'attribute_column2', 'area_sq_km']]\n",
    "\n",
    "# # Export the new DataFrame to a CSV file\n",
    "# csv_output_path = 'output.csv'\n",
    "# new_df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "# print(f'CSV file saved at: {csv_output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {\n",
    "    'DTCODE11.x': 'DTNAME',\n",
    "    'State code': 'State_code',\n",
    "    'State Name': 'State',\n",
    "    'State cens':'State_census_code',\n",
    "    'District c':'District_census_code', \n",
    "    'District N':'District', \n",
    "    'District_1':'District_code', \n",
    "    'SubDistric':'SubDistric', \n",
    "    'Subdistr_1':'Subdistrict',\n",
    "    'Subdistr_2':'Subdistr_2', \n",
    "    'Village co':'village_code', \n",
    "    'Village Na':'Village', \n",
    "    'Block code':'Block_code', \n",
    "    'Block Name':'Block',\n",
    "}\n",
    "\n",
    "gdf.rename(columns=column_mapping, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = gdf[['DTNAME',\t'State_code', 'State', 'State_census_code', 'District_census_code',\t'District', \n",
    "              'District_code', 'SubDistric', 'Subdistrict', 'Subdistr_2', 'village_code', 'Village', 'Block_code', 'Block', 'area_sq_km']]\n",
    "\n",
    "# # Export the new DataFrame to a CSV file\n",
    "# csv_output_path = '/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/ma2020/area.csv'\n",
    "# new_df.to_csv(csv_output_path, index=False)\n",
    "\n",
    "# print(f'CSV file saved at: {csv_output_path}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
