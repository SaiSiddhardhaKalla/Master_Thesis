{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF to CSV (New)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test\n",
    "\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your PDF file\n",
    "pdf_path = \"/Volumes/Ping Pong/TS2/00194909_f_20181023143245.pdf\"\n",
    "\n",
    "\n",
    "# Open the PDF file using pdfplumber\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    start_page = None\n",
    "    dfs = []\n",
    "\n",
    "    # Iterate over each page in the PDF\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        page_text = page.extract_text()\n",
    "        \n",
    "        # Check if \"1. Location Parameters\" is present in the page text\n",
    "        if \"1. Location Parameters\" in page_text:\n",
    "            start_page = i\n",
    "            break\n",
    "\n",
    "    if start_page is not None:\n",
    "        # Read the PDF file again, starting from the identified page\n",
    "        for page in pdf.pages[start_page:]:\n",
    "            tables = page.extract_tables()\n",
    "            for table in tables:\n",
    "                # Generate unique column names for each table\n",
    "                columns = [f\"Column_{i}\" for i in range(len(table[0]))]\n",
    "                df = pd.DataFrame(table[0:], columns=columns)\n",
    "                dfs.append(df)\n",
    "\n",
    "        if dfs:\n",
    "            # Concatenate all tables into a single DataFrame\n",
    "            combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "            # Shift first three rows to column 2\n",
    "            combined_df.loc[0:3, 'Column_1'] = combined_df['Column_0']\n",
    "            \n",
    "            # Drop unnecessary columns\n",
    "            combined_df = combined_df.drop(columns=[\"Column_0\",\"Column_3\", \"Column_7\", \"Column_8\"])\n",
    "\n",
    "            # Drop empty rows\n",
    "            combined_df = combined_df.dropna(how='all')\n",
    "\n",
    "            # Remove line breaks\n",
    "            combined_df = combined_df.replace('\\n', ' ', regex=True)\n",
    "\n",
    "            # Save the combined DataFrame as a CSV file\n",
    "            output_csv = \"output.csv\"\n",
    "            combined_df.to_csv(output_csv, header=None,index=False)\n",
    "            print(f\"Tables saved in {output_csv}\")\n",
    "        else:\n",
    "            print(\"No tables found in the PDF.\")\n",
    "    else:\n",
    "        print(\"Unable to find '1. Location Parameters' in the PDF.\")\n",
    "\n",
    "# print(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loop Test\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "## Change folder path and output_csv for each run\n",
    "# Path to the folder containing the files\n",
    "folder_path = \"/Users/sid/Documents/MA/AN/\"\n",
    "\n",
    "# Get a list of files in the folder\n",
    "file_list = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined results\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a dictionary to store the shape counts\n",
    "shape_counts = {}\n",
    "\n",
    "# Loop over each file\n",
    "for file_path in file_list:\n",
    "    # Read the file into a DataFrame\n",
    "    df2 = pd.read_csv(file_path)\n",
    "\n",
    "    ## State info\n",
    "    df4 = df2.drop(df2.index[3:], inplace=False) # only keep required rows\n",
    "    df4 = df4.iloc[:, [0, 1]] # drops unnecesary columns\n",
    "    df4 = df4.transpose().reset_index()\n",
    "    df4.columns = df4.iloc[0]\n",
    "    df4 = df4[1:]  # Remove the header row\n",
    "\n",
    "    ## District info\n",
    "    df5 = df2.drop(df2.index[2:], inplace=False)\n",
    "    df5 = df5.iloc[:, [2, 4]]\n",
    "    df5 = df5.transpose().reset_index()\n",
    "    df5.columns = df5.iloc[0]\n",
    "    df5 = df5[1:]  # Remove the header row\n",
    "\n",
    "    ## Village Parameters\n",
    "    df3 = df2.iloc[4:].transpose()\n",
    "    # Extract the header row and set it as the column names\n",
    "    df3.columns = df3.iloc[0]\n",
    "    df3 = df3[1:]  # Remove the header row\n",
    "    df3 = df3.iloc[1:]\n",
    "\n",
    "    df3 = df3.reset_index(drop=True)\n",
    "\n",
    "    # # Remove row 1,3\n",
    "    rows_to_drop = [0,2] \n",
    "    df3 = df3.drop(rows_to_drop)\n",
    "    df3 = df3.fillna(0)\n",
    "\n",
    "    # Reset the index\n",
    "    # df3 = df3.reset_index(drop=True)\n",
    "    # df3 = df3.dropna(how='all')\n",
    "    combined_df = pd.concat([df4, df5,df3], axis=1)\n",
    "    # print(combined_df.shape[1])\n",
    "\n",
    "    # Get the shape of combined_df\n",
    "    df_shape = combined_df.shape\n",
    "\n",
    "    # Increment the count for the shape in the dictionary\n",
    "    if df_shape in shape_counts:\n",
    "        shape_counts[df_shape] += 1\n",
    "    else:\n",
    "        shape_counts[df_shape] = 1\n",
    "\n",
    "    if combined_df.shape[1] == 54:\n",
    "        final_df = final_df.append(combined_df, ignore_index=True)\n",
    "     \n",
    "output_csv = \"/Users/sid/Documents/MA_20/AN.csv\"\n",
    "final_df.to_csv(output_csv, index=False)\n",
    "print(f\"Combined results saved in {output_csv}\")\n",
    "\n",
    "# Print the shape counts\n",
    "print(\"Shape Counts:\")\n",
    "for shape, count in shape_counts.items():\n",
    "    print(f\"Shape: {shape}, Count: {count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF to CSV loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2020 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# Folder path containing the PDF files\n",
    "folder_path = \"/Users/sid/Documents/MA/TN/\"\n",
    "\n",
    "# Output folder path for saving CSV files\n",
    "output_folder = \"/Users/sid/Documents/MA_20/TN/\"\n",
    "\n",
    "# Get a list of PDF files in the folder\n",
    "pdf_files = glob.glob(folder_path + \"*.pdf\")\n",
    "\n",
    "# Function to shift data by 2 columns if Column 3 is not empty. Some pdfs formatting causes page 3 data to be misinterpretted. \n",
    "def shift_data(row):\n",
    "    if row.name >= 30 and pd.notnull(row[3]):  # Check if row index is >= 30 and Column 3 is not empty\n",
    "        row[5:] = row[3:].values[:-2]  # Shift the data in columns 3 and onward by 2 positions\n",
    "    return row\n",
    "\n",
    "# Loop over each PDF file\n",
    "for pdf_path in pdf_files:\n",
    "    # print(\"Processing:\", pdf_path)\n",
    "\n",
    "    # Open the PDF file using pdfplumber\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        start_pages = []\n",
    "        dfs = []\n",
    "\n",
    "        # Iterate over each page in the PDF\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            page_text = page.extract_text()\n",
    "\n",
    "            # Check if \"1. Location Parameters\" is present in the page text\n",
    "            if \"1. Location Parameters\" in page_text:\n",
    "                start_pages.append(i)\n",
    "\n",
    "        if start_pages:\n",
    "            # Add the last page of the PDF as an end point to the list of start pages\n",
    "            # This helps handle the case where the last \"1. Location Parameters\" occurs at the end of the PDF\n",
    "            start_pages.append(len(pdf.pages))\n",
    "\n",
    "            # Iterate over each start page\n",
    "            for j in range(len(start_pages) - 1):\n",
    "                temp_dfs = []\n",
    "                start_page = start_pages[j]\n",
    "                end_page = start_pages[j + 1]\n",
    "\n",
    "                # Read the PDF file again, starting from the identified start page and ending at the next \"1. Location Parameters\" page\n",
    "                for page in pdf.pages[start_page:end_page]:\n",
    "                    tables = page.extract_tables()\n",
    "                    for table in tables:\n",
    "                        # Generate unique column names for each table\n",
    "                        columns = [f\"Column_{i}\" for i in range(len(table[0]))]\n",
    "                        df = pd.DataFrame(table[0:], columns=columns)\n",
    "                        temp_dfs.append(df)\n",
    "\n",
    "                if temp_dfs:\n",
    "                    # Concatenate all tables into a single DataFrame\n",
    "                    combined_df = pd.concat(temp_dfs, ignore_index=True)\n",
    "\n",
    "                    combined_df = combined_df.apply(shift_data, axis=1)\n",
    "\n",
    "                    combined_df = combined_df.drop(combined_df.columns[[4, 5]], axis=1)\n",
    "\n",
    "                    # Shift first three rows to column 2\n",
    "                    # combined_df.loc[0:3, 'Column_1'] = combined_df['Column_0']\n",
    "\n",
    "                    # Drop unnecessary columns.\n",
    "                    # combined_df = combined_df.drop(columns=[\"Column_0\", \"Column_3\"]) #, \"Column_7\", \"Column_8\"\n",
    "\n",
    "                    # Drop empty rows\n",
    "                    # combined_df = combined_df.dropna(how='all')\n",
    "\n",
    "                    # Remove line breaks\n",
    "                    combined_df = combined_df.replace('\\n', ' ', regex=True)\n",
    "\n",
    "                    # Generate a unique output filename using a counter\n",
    "                    filename = os.path.splitext(os.path.basename(pdf_path))[0]  # Get the base filename of the PDF\n",
    "                    counter = 1\n",
    "                    while True:\n",
    "                        output_csv = os.path.join(output_folder, f\"{filename}_output_{counter}.csv\")\n",
    "                        if not os.path.exists(output_csv):\n",
    "                            break\n",
    "                        counter += 1\n",
    "\n",
    "                    output_csv = os.path.join(output_folder, f\"{filename}_output_{j + 1}.csv\")\n",
    "                    \n",
    "                    # Save the combined DataFrame as a CSV file\n",
    "                    combined_df.to_csv(output_csv, header=None, index=False)\n",
    "                    # print(f\"Tables saved in {output_csv}\")\n",
    "                else:\n",
    "                    print(\"No tables found in the PDF.\")\n",
    "        else:\n",
    "            print(\"Unable to find '1. Location Parameters' in the PDF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "# Folder path containing the PDF files\n",
    "folder_path = \"/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/MA/MA_18_pdf/NG/\"\n",
    "\n",
    "# Output folder path for saving CSV files\n",
    "output_folder = \"/Users/sid/Library/CloudStorage/OneDrive-DeakinUniversity/UDocs - D/DataSets/MA/MA_18/NG/\"\n",
    "\n",
    "# Get a list of PDF files in the folder\n",
    "pdf_files = glob.glob(folder_path + \"*.pdf\")\n",
    "\n",
    "# Function to shift data by 2 columns if Column 3 is not empty. Some pdfs formatting causes page 3 data to be misinterpretted. \n",
    "def shift_data(row):\n",
    "    if row.name >= 30 and pd.notnull(row[3]):  # Check if row index is >= 30 and Column 3 is not empty\n",
    "        row[5:] = row[3:].values[:-2]  # Shift the data in columns 3 and onward by 2 positions\n",
    "    return row\n",
    "\n",
    "# Get total number of PDF files\n",
    "total_pdfs = len(pdf_files)\n",
    "processed_pdfs = 0\n",
    "\n",
    "# Define the intervals at which to print progress notifications\n",
    "progress_intervals = [20, 40, 60, 80, 100]\n",
    "current_interval = 0\n",
    "\n",
    "\n",
    "# Loop over each PDF file\n",
    "for pdf_path in pdf_files:\n",
    "    # print(\"Processing:\", pdf_path)\n",
    "\n",
    "    # Open the PDF file using pdfplumber\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        start_pages = []\n",
    "        dfs = []\n",
    "\n",
    "        # Iterate over each page in the PDF\n",
    "        for i, page in enumerate(pdf.pages):\n",
    "            page_text = page.extract_text()\n",
    "\n",
    "            # Check if \"1. Location Parameters\" is present in the page text\n",
    "            if \"1. Location Parameters\" in page_text:\n",
    "                start_pages.append(i)\n",
    "\n",
    "        if start_pages:\n",
    "            # Add the last page of the PDF as an end point to the list of start pages\n",
    "            # This helps handle the case where the last \"1. Location Parameters\" occurs at the end of the PDF\n",
    "            start_pages.append(len(pdf.pages))\n",
    "\n",
    "            # Iterate over each start page\n",
    "            for j in range(len(start_pages) - 1):\n",
    "                temp_dfs = []\n",
    "                start_page = start_pages[j]\n",
    "                end_page = start_pages[j + 1]\n",
    "\n",
    "                # Read the PDF file again, starting from the identified start page and ending at the next \"1. Location Parameters\" page\n",
    "                for page in pdf.pages[start_page:end_page]:\n",
    "                    tables = page.extract_tables()\n",
    "                    for table in tables:\n",
    "                        # Generate unique column names for each table\n",
    "                        columns = [f\"Column_{i}\" for i in range(len(table[0]))]\n",
    "                        df = pd.DataFrame(table[0:], columns=columns)\n",
    "                        temp_dfs.append(df)\n",
    "\n",
    "                if temp_dfs:\n",
    "                    # Concatenate all tables into a single DataFrame\n",
    "                    combined_df = pd.concat(temp_dfs, ignore_index=True)\n",
    "\n",
    "                    combined_df = combined_df.apply(shift_data, axis=1)\n",
    "\n",
    "                    combined_df = combined_df.drop(combined_df.columns[[7, 8]], axis=1)\n",
    "\n",
    "                    # Shift first three rows to column 2\n",
    "                    # combined_df.loc[0:3, 'Column_1'] = combined_df['Column_0']\n",
    "\n",
    "                    # Drop unnecessary columns.\n",
    "                    # combined_df = combined_df.drop(columns=[\"Column_0\", \"Column_3\"]) #, \"Column_7\", \"Column_8\"\n",
    "\n",
    "                    # Drop empty rows\n",
    "                    # combined_df = combined_df.dropna(how='all')\n",
    "\n",
    "                    # Remove line breaks\n",
    "                    combined_df = combined_df.replace('\\n', ' ', regex=True)\n",
    "\n",
    "                    # Generate a unique output filename using a counter\n",
    "                    filename = os.path.splitext(os.path.basename(pdf_path))[0]  # Get the base filename of the PDF\n",
    "                    counter = 1\n",
    "                    while True:\n",
    "                        output_csv = os.path.join(output_folder, f\"{filename}_output_{counter}.csv\")\n",
    "                        if not os.path.exists(output_csv):\n",
    "                            break\n",
    "                        counter += 1\n",
    "\n",
    "                    output_csv = os.path.join(output_folder, f\"{filename}_output_{j + 1}.csv\")\n",
    "                    \n",
    "                    # Save the combined DataFrame as a CSV file\n",
    "                    combined_df.to_csv(output_csv, header=None, index=False)\n",
    "                    # print(f\"Tables saved in {output_csv}\")\n",
    "                    #     # Increment the processed PDFs counter\n",
    "                    # processed_pdfs += 1\n",
    "                    \n",
    "                    # # Calculate the progress percentage\n",
    "                    # progress_percentage = (processed_pdfs / total_pdfs) * 100\n",
    "                    \n",
    "                    # # Check if the current progress is greater than or equal to the next interval\n",
    "                    # if progress_percentage >= progress_intervals[current_interval]:\n",
    "                    #     # Print progress notification\n",
    "                    #     print(f\"{progress_percentage:.2f}% of PDFs converted to CSVs\")\n",
    "                    #     # Move to the next interval\n",
    "                    #     current_interval += 1\n",
    "                        \n",
    "                    # # Check if all intervals have been reached\n",
    "                    # if current_interval >= len(progress_intervals):\n",
    "                    #     break\n",
    "                else:\n",
    "                    print(\"No tables found in the PDF.\")\n",
    "        else:\n",
    "            print(\"Unable to find '1. Location Parameters' in the PDF.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completed correct code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "## Change folder path and output_csv for each run\n",
    "# Path to the folder containing the files\n",
    "folder_path = \"/Users/sid/Documents/MA_20/AN/\"\n",
    "\n",
    "# Get a list of files in the folder\n",
    "file_list = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "# Initialize an empty DataFrame to store the combined results\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "# Initialize a dictionary to store the shape counts\n",
    "shape_counts = {}\n",
    "\n",
    "# Loop over each file\n",
    "for file_path in file_list:\n",
    "    # Read the file into a DataFrame\n",
    "    df2 = pd.read_csv(file_path)\n",
    "\n",
    "    ## State info\n",
    "    df4 = df2.drop(df2.index[3:], inplace=False) # only keep required rows\n",
    "    df4 = df4.iloc[:, [0, 1]] # drops unnecesary columns\n",
    "    df4 = df4.transpose().reset_index()\n",
    "    df4.columns = df4.iloc[0]\n",
    "    df4 = df4[1:]  # Remove the header row\n",
    "    df4 = df4.reset_index(drop=True)\n",
    "\n",
    "    ## District info\n",
    "    df5 = df2.drop(df2.index[2:], inplace=False)\n",
    "    df5 = df5.iloc[:, [2, 3]] ## changed index, different from TS\n",
    "    df5 = df5.transpose().reset_index()\n",
    "    df5.columns = df5.iloc[0]\n",
    "    df5 = df5[1:]  # Remove the header row\n",
    "    df5 = df5.reset_index(drop=True)\n",
    "\n",
    "    ## Village Parameters\n",
    "    df3 = df2.drop(df2.columns[0], axis=1) ## Added this, different from TS code\n",
    "    df3 = df3.iloc[8:].transpose() ## if TS, df3 = df2.iloc\n",
    "    df3 = df3.dropna(axis=1, how='all')  ## Added this, different from TS code\n",
    "    # Extract the header row and set it as the column names\n",
    "    df3.columns = df3.iloc[0]\n",
    "    df3 = df3[1:]  # Remove the header row\n",
    "    df3 = df3.iloc[1:]\n",
    "\n",
    "    df3 = df3.reset_index(drop=True)\n",
    "\n",
    "    # # # Remove row 1,3\n",
    "    # rows_to_drop = [0,2] \n",
    "    # df3 = df3.drop(rows_to_drop)\n",
    "    # df3 = df3.fillna(0)\n",
    "\n",
    "    # Reset the index\n",
    "    # df3 = df3.reset_index(drop=True)\n",
    "    # df3 = df3.dropna(how='all')\n",
    "    combined_df = pd.concat([df4, df5,df3], axis=1)\n",
    "    # print(combined_df.shape[1])\n",
    "\n",
    "    # Get the shape of combined_df\n",
    "    df_shape = combined_df.shape\n",
    "\n",
    "    # Increment the count for the shape in the dictionary\n",
    "    if df_shape in shape_counts:\n",
    "        shape_counts[df_shape] += 1\n",
    "    else:\n",
    "        shape_counts[df_shape] = 1\n",
    "\n",
    "    # if combined_df.shape[1] == 54: ## Only use in 2018 data\n",
    "    final_df = final_df.append(combined_df, ignore_index=True)\n",
    "     \n",
    "output_csv = \"/Users/sid/Documents/MA_20/AN.csv\"\n",
    "final_df.to_csv(output_csv, index=False)\n",
    "print(f\"Combined results saved in {output_csv}\")\n",
    "\n",
    "# Print the shape counts\n",
    "print(\"Shape Counts:\")\n",
    "for shape, count in shape_counts.items():\n",
    "    print(f\"Shape: {shape}, Count: {count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Master Folder loop code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Path to the master folder containing subfolders\n",
    "master_folder = \"/Users/sid/Documents/MA_20/\"\n",
    "\n",
    "# Get a list of subfolders in the master folder\n",
    "subfolders = [subfolder for subfolder in os.listdir(master_folder) if os.path.isdir(os.path.join(master_folder, subfolder))]\n",
    "\n",
    "# Loop through each subfolder\n",
    "for subfolder in subfolders:\n",
    "    if subfolder != \"UP\":\n",
    "        print(f\"Skipping folder {subfolder}.\\n\")\n",
    "        continue  # Skip processing this folder and move to the next\n",
    "\n",
    "    folder_path = os.path.join(master_folder, subfolder)\n",
    "    \n",
    "    # Get a list of files in the subfolder\n",
    "    file_list = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "    print(f\"Beginning folder {subfolder}.\\n\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "    shape_counts = {}\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        df2 = pd.read_csv(file_path)\n",
    "\n",
    "        ## State info\n",
    "        df4 = df2.drop(df2.index[3:], inplace=False) # only keep required rows\n",
    "        df4 = df4.iloc[:, [0, 1]] # drops unnecesary columns\n",
    "        df4 = df4.transpose().reset_index()\n",
    "        df4.columns = df4.iloc[0]\n",
    "        df4 = df4[1:]  # Remove the header row\n",
    "        df4 = df4.reset_index(drop=True)\n",
    "\n",
    "        ## District info\n",
    "        df5 = df2.drop(df2.index[2:], inplace=False)\n",
    "        df5 = df5.iloc[:, [2, 3]] ## changed index, different from TS\n",
    "        df5 = df5.transpose().reset_index()\n",
    "        df5.columns = df5.iloc[0]\n",
    "        df5 = df5[1:]  # Remove the header row\n",
    "        df5 = df5.reset_index(drop=True)\n",
    "\n",
    "        ## Village Parameters\n",
    "        df3 = df2.drop(df2.columns[0], axis=1) ## Added this, different from TS code\n",
    "        df3 = df3.iloc[8:].transpose() ## if TS, df3 = df2.iloc\n",
    "        df3 = df3.dropna(axis=1, how='all')  ## Added this, different from TS code\n",
    "        # Extract the header row and set it as the column names\n",
    "        df3.columns = df3.iloc[0]\n",
    "        df3 = df3[1:]  # Remove the header row\n",
    "        df3 = df3.iloc[1:]\n",
    "\n",
    "        df3 = df3.reset_index(drop=True)\n",
    "\n",
    "        combined_df = pd.concat([df4, df5,df3], axis=1)\n",
    "\n",
    "        # Get the shape of combined_df\n",
    "        df_shape = combined_df.shape\n",
    "\n",
    "        # Increment the count for the shape in the dictionary\n",
    "        if df_shape in shape_counts:\n",
    "            shape_counts[df_shape] += 1\n",
    "        else:\n",
    "            shape_counts[df_shape] = 1\n",
    "\n",
    "        # if combined_df.shape[1] == 54: ## Only use in 2018 data\n",
    "        final_df = final_df.append(combined_df, ignore_index=True)\n",
    "\n",
    "    # Calculate time taken\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    \n",
    "    # Save combined results to CSV\n",
    "    output_csv = os.path.join(master_folder, f\"{subfolder}.csv\")\n",
    "    final_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Combined results for {subfolder} saved in {output_csv}\")\n",
    "\n",
    "    # Print shape counts\n",
    "    print(\"Shape Counts:\")\n",
    "    for shape, count in shape_counts.items():\n",
    "        print(f\"Shape: {shape}, Count: {count}\")\n",
    "\n",
    "    print(f\"Folder {subfolder} completed in {time_taken:.2f} seconds.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory containing CSV files\n",
    "csv_directory = \"/Users/sid/Documents/MA_20/\"\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(csv_directory) if file.endswith(\".csv\")]\n",
    "\n",
    "# Dictionary to store column sets\n",
    "column_sets = {}\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(csv_directory, csv_file)\n",
    "    \n",
    "    # Read the CSV file using pandas\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get the shape of the DataFrame\n",
    "    shape = df.shape\n",
    "    \n",
    "    # Get the set of columns\n",
    "    columns = tuple(df.columns)\n",
    "    \n",
    "    # Store the columns and shape in the dictionary\n",
    "    column_sets[file_path] = (shape, columns)\n",
    "\n",
    "# Create a dictionary to group files with same columns\n",
    "column_groups = {}\n",
    "\n",
    "# Group files with same columns\n",
    "for file, (shape, columns) in column_sets.items():\n",
    "    columns_key = tuple(columns)\n",
    "    \n",
    "    if columns_key in column_groups:\n",
    "        column_groups[columns_key].append((file, shape))\n",
    "    else:\n",
    "        column_groups[columns_key] = [(file, shape)]\n",
    "\n",
    "# Print grouped information\n",
    "for columns_key, files_and_shapes in column_groups.items():\n",
    "    if len(files_and_shapes) > 1:\n",
    "        file_info = [f\"{os.path.basename(file)} ({shape})\" for file, shape in files_and_shapes]\n",
    "        print(f\"Files {', '.join(file_info)} have the same columns.\")\n",
    "    else:\n",
    "        file_info = files_and_shapes[0]\n",
    "        print(f\"File {os.path.basename(file_info[0])} ({file_info[1]}) has different columns.\")\n",
    "\n",
    "print(\"Comparison finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory containing CSV files\n",
    "csv_directory = \"/Users/sid/Documents/MA_20/\"\n",
    "\n",
    "# Get a list of all CSV files in the directory\n",
    "csv_files = [file for file in os.listdir(csv_directory) if file.endswith(\".csv\")]\n",
    "\n",
    "# Dictionary to store column sets\n",
    "column_sets = {}\n",
    "\n",
    "# Loop through each CSV file\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(csv_directory, csv_file)\n",
    "    \n",
    "    # Read the CSV file using pandas\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Get the shape of the DataFrame\n",
    "    shape = df.shape\n",
    "    \n",
    "    # Get the set of columns\n",
    "    columns = tuple(df.columns)\n",
    "    \n",
    "    # Store the columns and shape in the dictionary\n",
    "    column_sets[file_path] = (shape, columns)\n",
    "\n",
    "# Create a dictionary to group files with same columns\n",
    "column_groups = {}\n",
    "\n",
    "# Group files with same columns\n",
    "for file, (shape, columns) in column_sets.items():\n",
    "    columns_key = tuple(columns)\n",
    "    \n",
    "    if columns_key in column_groups:\n",
    "        column_groups[columns_key].append((file, shape))\n",
    "    else:\n",
    "        column_groups[columns_key] = [(file, shape)]\n",
    "\n",
    "# Concatenate files with similar columns\n",
    "output_counter = 1\n",
    "for columns_key, files_and_shapes in column_groups.items():\n",
    "    if len(files_and_shapes) > 1:\n",
    "        # Extract paths of files with same columns\n",
    "        file_paths = [file for file, shape in files_and_shapes]\n",
    "        \n",
    "        # Read and concatenate the dataframes\n",
    "        dfs = [pd.read_csv(file_path) for file_path in file_paths]\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Create an output file name using incremental numbering\n",
    "        output_file = os.path.join(csv_directory, f\"{output_counter}.csv\")\n",
    "        \n",
    "        # Save the combined dataframe to the output file name\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Combined files with {columns_key} columns saved as {output_file}\")\n",
    "        \n",
    "        output_counter += 1\n",
    "\n",
    "print(\"Comparison and concatenation finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
